"""
é«˜çº§æ¨¡å‹è®­ç»ƒ - 2025ç‰ˆ
ä½¿ç”¨314æœŸæ•°æ®ï¼Œå¤šæ¨¡å‹é›†æˆæå‡é¢„æµ‹æˆåŠŸç‡
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from collections import Counter
import joblib
import warnings
warnings.filterwarnings('ignore')

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ XGBoostæœªå®‰è£…")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸ LightGBMæœªå®‰è£…")

try:
    from catboost import CatBoostRegressor
    CATBOOST_AVAILABLE = True
except:
    CATBOOST_AVAILABLE = False
    print("âš ï¸ CatBoostæœªå®‰è£…")


class AdvancedLuckyNumberPredictor:
    """é«˜çº§å¹¸è¿æ•°å­—é¢„æµ‹å™¨"""
    
    def __init__(self, sequence_length=15):
        self.sequence_length = sequence_length
        self.models = {}
        self.scaler = StandardScaler()
        self.element_numbers = {
            'é‡‘': [3, 4, 11, 12, 25, 26, 33, 34, 41, 42],
            'æœ¨': [7, 8, 15, 16, 23, 24, 37, 38, 45, 46],
            'æ°´': [13, 14, 21, 22, 29, 30, 43, 44],
            'ç«': [1, 2, 9, 10, 17, 18, 31, 32, 39, 40, 47, 48],
            'åœŸ': [5, 6, 19, 20, 27, 28, 35, 36, 49]
        }
        
    def create_features(self, numbers, elements):
        """åˆ›å»ºé«˜çº§ç‰¹å¾"""
        features = []
        
        # 1. åŸºç¡€åºåˆ—ç‰¹å¾ï¼ˆæœ€è¿‘Nä¸ªæ•°å­—ï¼‰
        recent = list(numbers[-self.sequence_length:])
        features.extend(recent)
        
        # 2. ç»Ÿè®¡ç‰¹å¾
        features.append(np.mean(recent))  # å¹³å‡å€¼
        features.append(np.std(recent))   # æ ‡å‡†å·®
        features.append(np.max(recent))   # æœ€å¤§å€¼
        features.append(np.min(recent))   # æœ€å°å€¼
        features.append(np.median(recent)) # ä¸­ä½æ•°
        
        # 3. è¶‹åŠ¿ç‰¹å¾
        features.append(recent[-1] - recent[0])  # æ€»è¶‹åŠ¿
        features.append(recent[-1] - recent[-2] if len(recent) > 1 else 0)  # çŸ­æœŸè¶‹åŠ¿
        
        # 4. å·®åˆ†ç‰¹å¾
        diffs = [recent[i] - recent[i-1] for i in range(1, len(recent))]
        features.append(np.mean(diffs))
        features.append(np.std(diffs))
        features.append(max(diffs) if diffs else 0)
        features.append(min(diffs) if diffs else 0)
        
        # 5. åŒºé—´åˆ†å¸ƒç‰¹å¾
        bins = [(1, 10), (11, 20), (21, 29), (30, 39), (40, 49)]
        for low, high in bins:
            count = sum(1 for n in recent if low <= n <= high)
            features.append(count)
            features.append(count / len(recent))  # æ¯”ä¾‹
        
        # 6. å¥‡å¶ç‰¹å¾
        odd_count = sum(1 for n in recent if n % 2 == 1)
        features.append(odd_count)
        features.append(odd_count / len(recent))
        
        # 7. äº”è¡Œåˆ†å¸ƒç‰¹å¾
        for element, nums in self.element_numbers.items():
            count = sum(1 for n in recent if n in nums)
            features.append(count)
        
        # 8. é¢‘ç‡ç‰¹å¾ï¼ˆæœ€è¿‘10æœŸï¼‰
        recent_10 = numbers[-10:] if len(numbers) >= 10 else numbers
        freq = Counter(recent_10)
        most_common = freq.most_common(5)
        for i in range(5):
            if i < len(most_common):
                features.append(most_common[i][0])
                features.append(most_common[i][1])
            else:
                features.append(0)
                features.append(0)
        
        # 9. è·ç¦»ç‰¹å¾ï¼ˆä¸æœ€è¿‘æ•°å­—çš„è·ç¦»ï¼‰
        last_num = recent[-1]
        for num in range(1, 50):
            features.append(abs(num - last_num))
        
        # 10. å‘¨æœŸç‰¹å¾ï¼ˆä½ç½®ï¼‰
        features.append(len(numbers) % 7)  # å‘¨å‡ 
        features.append(len(numbers) % 30) # æœˆä¸­ä½ç½®
        
        return np.array(features)
    
    def prepare_dataset(self, df):
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
        numbers = df['number'].values
        elements = df['element'].values if 'element' in df.columns else [None] * len(numbers)
        
        X, y = [], []
        
        # åˆ›å»ºåºåˆ—æ•°æ®
        for i in range(self.sequence_length, len(numbers)):
            # ä½¿ç”¨å‰iæœŸæ•°æ®åˆ›å»ºç‰¹å¾
            train_numbers = numbers[:i]
            train_elements = elements[:i]
            
            features = self.create_features(train_numbers, train_elements)
            target = numbers[i]
            
            X.append(features)
            y.append(target)
        
        return np.array(X), np.array(y)
    
    def train_models(self, X_train, y_train):
        """è®­ç»ƒå¤šä¸ªæ¨¡å‹"""
        print("\n" + "="*80)
        print("å¼€å§‹è®­ç»ƒé«˜çº§æ¨¡å‹é›†æˆ...")
        print("="*80)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        models_to_train = []
        
        # 1. Random Forest
        models_to_train.append(('RandomForest', RandomForestRegressor(
            n_estimators=200,
            max_depth=15,
            min_samples_split=3,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )))
        
        # 2. Gradient Boosting
        models_to_train.append(('GradientBoosting', GradientBoostingRegressor(
            n_estimators=150,
            learning_rate=0.05,
            max_depth=8,
            min_samples_split=3,
            random_state=42
        )))
        
        # 3. Extra Trees
        models_to_train.append(('ExtraTrees', ExtraTreesRegressor(
            n_estimators=200,
            max_depth=15,
            min_samples_split=3,
            random_state=42,
            n_jobs=-1
        )))
        
        # 4. XGBoost (å¦‚æœå¯ç”¨)
        if XGBOOST_AVAILABLE:
            models_to_train.append(('XGBoost', xgb.XGBRegressor(
                n_estimators=150,
                learning_rate=0.05,
                max_depth=8,
                random_state=42,
                n_jobs=-1
            )))
        
        # 5. LightGBM (å¦‚æœå¯ç”¨)
        if LIGHTGBM_AVAILABLE:
            models_to_train.append(('LightGBM', lgb.LGBMRegressor(
                n_estimators=150,
                learning_rate=0.05,
                max_depth=8,
                random_state=42,
                n_jobs=-1,
                verbose=-1
            )))
        
        # 6. CatBoost (å¦‚æœå¯ç”¨)
        if CATBOOST_AVAILABLE:
            models_to_train.append(('CatBoost', CatBoostRegressor(
                iterations=150,
                learning_rate=0.05,
                depth=8,
                random_state=42,
                verbose=False
            )))
        
        # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        for name, model in models_to_train:
            print(f"\nè®­ç»ƒ {name}...")
            try:
                model.fit(X_train_scaled, y_train)
                self.models[name] = model
                print(f"âœ… {name} è®­ç»ƒå®Œæˆ")
            except Exception as e:
                print(f"âŒ {name} è®­ç»ƒå¤±è´¥: {str(e)}")
        
        print(f"\næˆåŠŸè®­ç»ƒ {len(self.models)} ä¸ªæ¨¡å‹")
    
    def predict_top_k(self, numbers, elements, k=15):
        """é¢„æµ‹TOP Kä¸ªæ•°å­—"""
        features = self.create_features(numbers, elements)
        features_scaled = self.scaler.transform(features.reshape(1, -1))
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        all_predictions = {}
        
        for name, model in self.models.items():
            try:
                pred = model.predict(features_scaled)[0]
                # å››èˆäº”å…¥åˆ°æœ€è¿‘çš„æ•´æ•°
                pred_int = int(round(pred))
                # é™åˆ¶åœ¨1-49èŒƒå›´å†…
                pred_int = max(1, min(49, pred_int))
                
                # è®°å½•é¢„æµ‹
                if pred_int not in all_predictions:
                    all_predictions[pred_int] = 0
                all_predictions[pred_int] += 1
            except Exception as e:
                print(f"âš ï¸ {name} é¢„æµ‹å¤±è´¥: {str(e)}")
        
        # åŸºäºé¢‘ç‡å’Œç»Ÿè®¡æ–¹æ³•è¡¥å……é¢„æµ‹
        recent_10 = numbers[-10:]
        freq = Counter(recent_10)
        
        # ç»Ÿè®¡æ–¹æ³•1: é«˜é¢‘æ•°å­—
        for num, count in freq.most_common(10):
            if num not in all_predictions:
                all_predictions[num] = count * 0.5
            else:
                all_predictions[num] += count * 0.5
        
        # ç»Ÿè®¡æ–¹æ³•2: åŒºåŸŸåˆ†å¸ƒ
        extreme_count = sum(1 for n in recent_10 if n <= 10 or n >= 40)
        if extreme_count > 5:  # æç«¯å€¼è¶‹åŠ¿
            for n in list(range(1, 11)) + list(range(40, 50)):
                if n not in recent_10[-3:]:  # æ’é™¤æœ€è¿‘3æœŸ
                    if n not in all_predictions:
                        all_predictions[n] = 0.3
                    else:
                        all_predictions[n] += 0.3
        
        # æ’åºå¹¶è¿”å›TOP K
        sorted_predictions = sorted(all_predictions.items(), key=lambda x: x[1], reverse=True)
        top_k_numbers = [num for num, score in sorted_predictions[:k]]
        
        return top_k_numbers
    
    def save_models(self, prefix='advanced'):
        """ä¿å­˜æ¨¡å‹"""
        import os
        from datetime import datetime
        
        os.makedirs('models', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜scaler
        scaler_path = f'models/{prefix}_scaler_{timestamp}.pkl'
        joblib.dump(self.scaler, scaler_path)
        print(f"âœ… Scaler å·²ä¿å­˜: {scaler_path}")
        
        # ä¿å­˜æ¯ä¸ªæ¨¡å‹
        for name, model in self.models.items():
            model_path = f'models/{prefix}_{name}_{timestamp}.pkl'
            joblib.dump(model, model_path)
            print(f"âœ… {name} å·²ä¿å­˜: {model_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("é«˜çº§å¹¸è¿æ•°å­—é¢„æµ‹æ¨¡å‹è®­ç»ƒ - åŸºäº314æœŸæ•°æ®")
    print("="*80)
    
    # åŠ è½½æ•°æ®
    print("\nåŠ è½½æ•°æ®...")
    df = pd.read_csv('data/lucky_numbers.csv', encoding='utf-8-sig')
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(df)} æœŸ")
    print(f"   æ—¥æœŸèŒƒå›´: {df.iloc[0]['date']} è‡³ {df.iloc[-1]['date']}")
    print(f"   æœ€æ–°å·ç : {df.iloc[-1]['number']}")
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = AdvancedLuckyNumberPredictor(sequence_length=15)
    
    # å‡†å¤‡æ•°æ®é›†
    print("\nå‡†å¤‡è®­ç»ƒæ•°æ®é›†...")
    X, y = predictor.prepare_dataset(df)
    print(f"âœ… ç‰¹å¾ç»´åº¦: {X.shape}")
    print(f"   æ ·æœ¬æ•°: {len(X)}")
    print(f"   ç‰¹å¾æ•°: {X.shape[1]}")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.1, random_state=42, shuffle=False
    )
    print(f"\nè®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
    
    # è®­ç»ƒæ¨¡å‹
    predictor.train_models(X_train, y_train)
    
    # éªŒè¯æ¨¡å‹
    print("\n" + "="*80)
    print("æ¨¡å‹éªŒè¯ - æœ€è¿‘10æœŸé¢„æµ‹")
    print("="*80)
    
    # ä½¿ç”¨æœ€è¿‘10æœŸè¿›è¡ŒéªŒè¯
    validation_results = []
    for i in range(10):
        idx = -(10 - i)
        test_df = df.iloc[:idx]
        actual = df.iloc[idx]['number']
        
        numbers = test_df['number'].values
        elements = test_df['element'].values
        
        top15 = predictor.predict_top_k(numbers, elements, k=15)
        top10 = top15[:10]
        top5 = top15[:5]
        
        hit_top5 = actual in top5
        hit_top10 = actual in top10
        hit_top15 = actual in top15
        
        validation_results.append({
            'actual': actual,
            'top5': top5,
            'top15': top15,
            'hit_top5': hit_top5,
            'hit_top10': hit_top10,
            'hit_top15': hit_top15
        })
        
        status = "âœ…" if hit_top15 else "âŒ"
        rank = top15.index(actual) + 1 if hit_top15 else "-"
        print(f"{status} æœŸ{i+1}: å®é™…={actual:2d} | TOP5={top5} | å‘½ä¸­={rank}")
    
    # ç»Ÿè®¡éªŒè¯ç»“æœ
    print("\n" + "="*80)
    print("éªŒè¯ç»“æœç»Ÿè®¡")
    print("="*80)
    
    top5_hits = sum(1 for r in validation_results if r['hit_top5'])
    top10_hits = sum(1 for r in validation_results if r['hit_top10'])
    top15_hits = sum(1 for r in validation_results if r['hit_top15'])
    
    print(f"TOP 5  å‘½ä¸­: {top5_hits}/10 = {top5_hits*10}%")
    print(f"TOP 10 å‘½ä¸­: {top10_hits}/10 = {top10_hits*10}%")
    print(f"TOP 15 å‘½ä¸­: {top15_hits}/10 = {top15_hits*10}%")
    
    # é¢„æµ‹ä¸‹ä¸€æœŸ
    print("\n" + "="*80)
    print("é¢„æµ‹ä¸‹ä¸€æœŸ (2025/12/14)")
    print("="*80)
    
    numbers = df['number'].values
    elements = df['element'].values
    
    top15 = predictor.predict_top_k(numbers, elements, k=15)
    print(f"\nğŸ¯ TOP 5:  {top15[:5]}")
    print(f"ğŸ“Š TOP 10: {top15[:10]}")
    print(f"ğŸ“‹ TOP 15: {top15}")
    
    # ä¿å­˜æ¨¡å‹
    print("\n" + "="*80)
    print("ä¿å­˜æ¨¡å‹")
    print("="*80)
    predictor.save_models(prefix='advanced_v2')
    
    print("\n" + "="*80)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("="*80)


if __name__ == '__main__':
    main()
